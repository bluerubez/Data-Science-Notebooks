
Stock Market Data Munging with Apache Spark
In this Ipython notebook I will show how to get stock market "End of Day" data. Then I will construct a formidable dataset and analyze the dataset as an Apache Spark Dataframe.
In [1]:

import os, pandas, requests
from pyspark import SparkContext, SparkConf
from pyspark.sql import HiveContext, SQLContext
import pandas as pd
from IPython.display import clear_output

Yahoo has the best API to get bulk stock market data. With this url one can get the entire history of a stock by appending a ticker name to the end of this string. The data comes back in csv format.
In [2]:

base_url = 'http://ichart.finance.yahoo.com/table.csv?s='

Two small functions to expedite the bulk requests process
In [3]:

def make_url(ticker_symbol):
    return base_url + ticker_symbol
def make_filename(ticker_symbol):
    return ticker_symbol + '.csv'

In [4]:

os.chdir('NYSE')

From http://www.nasdaq.com/screening/company-list.aspx I downloaded a file that contains all the tickers in the New York Stock Exchange. By loading the file as a DictReader from the csv module I was able to retrieve the first column 'Symbol', which contains the tickers, just like a table. Then I interated over the rows and sent requests to the baseurl. This took a while to run...
In [12]:

import csv, urllib
with open('//home//bluerubez//Downloads//companylist.csv') as csvfile:
    reader = csv.DictReader(csvfile)
    for row in reader:
        urllib.urlretrieve(make_url(row['Symbol']),make_filename(row['Symbol']))
clear_output()

I had to remove a bunch of returns that were saved as csv files but were actually bogus html requests due to the fact that yahoo did not recognize that particular ticker. They don't have them all accessible from this API.
In [9]:

l = os.listdir('.')
for i in l:
    if '^' in i:
        os.remove(i)

In [11]:

l = os.listdir('.')

for item in l:
    with open(item) as f:
        line = f.readline()
        if 'doctype' in line:
            os.remove(item)
    

In each csv the ticker was in the header. So all i did here was pull down the ticker to each row of all the files
In [15]:

import re,os,csv
l = os.listdir('/home/bluerubez/Downloads/spark-1.5.2//NYSE')
os.chdir('/home/bluerubez/Downloads/spark-1.5.2//NYSE')

for item in l:
    result = re.search('(.*).csv', item)
    if result is not None:
        tick = result.group(1);print tick
        with open(item, 'rb') as input, open(tick+'1'+'.csv', 'wb') as output:
            reader = csv.reader(input, delimiter = ',')
            writer = csv.writer(output, delimiter = ',')
            all = []
            row = next(reader)
            row.insert(0, 'Ticker')
            all.append(row)
            for k, row in enumerate(reader):
                all.append([tick] + row)
            writer.writerows(all)
clear_output()            

Here I take all the csv files and concatenate them together to form a master file containing all the data so i can load it into a data frame
In [19]:

import os, sys
os.chdir('/home/bluerubez/Downloads/spark-1.5.2/NYSE')
l = os.listdir('.')
target = 'MasterNYSEEOD.csv'

for item in l:
    with open(target, 'a') as tar:
        with open(item) as f:
            s = str(f.readline())
            for line in f:
                tar.write(line)
tar.close()

OK here starts the spark!
In [2]:

sql_sc = SQLContext(sc)

In [3]:

pandas_df = pd.read_csv('NYSE.csv')

In [4]:

spark_df = sql_sc.createDataFrame(pandas_df)

In [6]:

spark_df.show()

+----------+------+--------------------+--------+--------------------+---------------+
|Unnamed: 0|Ticker|                Name|Exchange|       Category Name|Category Number|
+----------+------+--------------------+--------+--------------------+---------------+
|         0|AUB.AX|Austbrokers Holdi...|    NYSE|Accident & Health...|            431|
|         1|  GLRE|Greenlight Capita...|    NYSE|Accident & Health...|            431|
|         2|   SFG|StanCorp Financia...|    NYSE|Accident & Health...|            431|
|         3|  AMIC|American Independ...|    NYSE|Accident & Health...|            431|
|         4|   GTS|Triple-S Manageme...|    NYSE|Accident & Health...|            431|
|         5| LRE.L|Lancashire Holdin...|    NYSE|Accident & Health...|            431|
|         6|   EIG|Employers Holding...|    NYSE|Accident & Health...|            431|
|         7|   AIZ|       Assurant Inc.|    NYSE|Accident & Health...|            431|
|         8|NHF.AX|NIB Holdings Limited|    NYSE|Accident & Health...|            431|
|         9|   PRA|ProAssurance Corp...|    NYSE|Accident & Health...|            431|
|        10|  TRUP|     Trupanion, Inc.|    NYSE|Accident & Health...|            431|
|        11|   AFL|          AFLAC Inc.|    NYSE|Accident & Health...|            431|
|        12|MIC.TO|Genworth MI Canad...|    NYSE|Accident & Health...|            431|
|        13|   CNO|CNO Financial Gro...|    NYSE|Accident & Health...|            431|
|        14|   UNM|          Unum Group|    NYSE|Accident & Health...|            431|
|        15|   FAC|First Acceptance ...|    NYSE|Accident & Health...|            431|
|        16|  SCRI|Social Reality, Inc.|    NYSE|Advertising Agencies|            720|
|        17|DCS.MU|           JC DECAUX|    NYSE|Advertising Agencies|            720|
|        18| XM8.F|         CLEAR MEDIA|    NYSE|Advertising Agencies|            720|
|        19|  XBTC|        Xhibit Corp.|    NYSE|Advertising Agencies|            720|
+----------+------+--------------------+--------+--------------------+---------------+
only showing top 20 rows


In [7]:

spark_df.select('Name','Exchange').show()

+--------------------+--------+
|                Name|Exchange|
+--------------------+--------+
|Austbrokers Holdi...|    NYSE|
|Greenlight Capita...|    NYSE|
|StanCorp Financia...|    NYSE|
|American Independ...|    NYSE|
|Triple-S Manageme...|    NYSE|
|Lancashire Holdin...|    NYSE|
|Employers Holding...|    NYSE|
|       Assurant Inc.|    NYSE|
|NIB Holdings Limited|    NYSE|
|ProAssurance Corp...|    NYSE|
|     Trupanion, Inc.|    NYSE|
|          AFLAC Inc.|    NYSE|
|Genworth MI Canad...|    NYSE|
|CNO Financial Gro...|    NYSE|
|          Unum Group|    NYSE|
|First Acceptance ...|    NYSE|
|Social Reality, Inc.|    NYSE|
|           JC DECAUX|    NYSE|
|         CLEAR MEDIA|    NYSE|
|        Xhibit Corp.|    NYSE|
+--------------------+--------+
only showing top 20 rows


In [8]:

spark_df.collect()
clear_output()

In [9]:

spark_df.count()

Out[9]:

25242

In [12]:

spark_df.filter(spark_df['Ticker'] == 'GOOG').show()

+----------+------+-----------+--------+--------------------+---------------+
|Unnamed: 0|Ticker|       Name|Exchange|       Category Name|Category Number|
+----------+------+-----------+--------+--------------------+---------------+
|      6753|  GOOG|Google Inc.|    NYSE|Internet Informat...|            851|
+----------+------+-----------+--------+--------------------+---------------+


In [5]:

sqlContext = SQLContext(sc)
pandas_df = pd.read_csv('MasterNYSEEOD.csv')
stocks_df = sqlContext.createDataFrame(pandas_df)

In [6]:

stocks_df.show()

+------+----------+-----+-----+-----+-----+------+---------+
|Ticker|      Date| Open| High|  Low|Close|Volume|Adj Close|
+------+----------+-----+-----+-----+-----+------+---------+
|   CBZ|2015-12-10|10.59|10.63| 10.5|10.55|135700|    10.55|
|   CBZ|2015-12-09|10.72|10.74| 10.5| 10.6|220400|     10.6|
|   CBZ|2015-12-08|10.61|10.76|10.58|10.72|196500|    10.72|
|   CBZ|2015-12-07| 10.7|10.85|10.68|10.76|407800|    10.76|
|   CBZ|2015-12-04|10.68|10.79|10.64|10.75|185400|    10.75|
|   CBZ|2015-12-03|10.87|10.87| 10.6|10.66|187200|    10.66|
|   CBZ|2015-12-02|10.87|10.92|10.81|10.84|185800|    10.84|
|   CBZ|2015-12-01|10.72|10.87|10.66|10.86|291800|    10.86|
|   CBZ|2015-11-30|10.59|10.72|10.56|10.71|356200|    10.71|
|   CBZ|2015-11-27| 10.5|10.59|10.45|10.59|147600|    10.59|
|   CBZ|2015-11-25| 10.5| 10.6|10.48|10.49|176800|    10.49|
|   CBZ|2015-11-24|10.39|10.52|10.28|10.52|210900|    10.52|
|   CBZ|2015-11-23|10.32|10.47|10.32|10.42|200100|    10.42|
|   CBZ|2015-11-20| 10.4|10.48|10.34| 10.4|338200|     10.4|
|   CBZ|2015-11-19|10.35|10.37|10.27|10.35|176600|    10.35|
|   CBZ|2015-11-18|10.18|10.39|10.13|10.37|335100|    10.37|
|   CBZ|2015-11-17|10.21|10.22|10.07|10.15|228400|    10.15|
|   CBZ|2015-11-16|10.22|10.26|10.14|10.21|240900|    10.21|
|   CBZ|2015-11-13|10.25|10.28|10.15|10.23|262300|    10.23|
|   CBZ|2015-11-12|10.37|10.38|10.22|10.29|397900|    10.29|
+------+----------+-----+-----+-----+-----+------+---------+
only showing top 20 rows


In [5]:

stocks_df.groupBy('Ticker').count().show()

+--------------+-----+
|        Ticker|count|
+--------------+-----+
|           ADC| 5454|
|           INF| 1079|
|            MT| 4617|
|          FBHS| 1066|
|           EOS| 2738|
|          CNCO| 1433|
|ETX           |  685|
|          GLOB|  354|
|           EOT| 1649|
|            GG| 5115|
|           DTZ| 1008|
|           AWP| 2174|
|           STK| 1520|
|          EROS|  523|
|          DLNG|  523|
|           BEN| 7883|
|           AWR| 6481|
|           DAR| 5353|
|           CFX| 1913|
|          FELP|  375|
+--------------+-----+
only showing top 20 rows


This is how you can register a datatable as an sql-like table. Then you can issue sql commands on the table
In [7]:

stocks_df.registerTempTable("stocks")

In [16]:

sqlContext.sql("SELECT * FROM stocks WHERE Volume>100000000").show()

+------+----------+------------------+------------------+-------------------+------------------+---------+------------------+
|Ticker|      Date|              Open|              High|                Low|             Close|   Volume|         Adj Close|
+------+----------+------------------+------------------+-------------------+------------------+---------+------------------+
|    GM|2010-11-19|         34.150002|              34.5| 33.110001000000004|34.259997999999996|107842000|         32.089555|
|    GM|2010-11-18|              35.0|35.990002000000004| 33.889998999999996|         34.189999|457044300|32.023990000000005|
|   AVP|1988-05-12|24.125089000000003|         24.250102|          24.000076|         24.000076|318182400|          1.020799|
|   AVP|1988-02-12|         24.000076|         24.250102|          23.875063|24.125089000000003|165642400|          1.004057|
|   GGP|2013-12-09|             21.23|             21.52|          21.040001|             21.17|116835700|          20.10427|
|   GGP|2009-04-17|               0.6|0.8599950000000001|0.49999799999999994|          0.620005|103482100|          0.415684|
|  ABBV|2014-10-15|         52.330002|         55.720001|          52.060001|         54.630001|122740200|         52.892596|
|    BP|2010-06-17|         32.200001|32.459998999999996|              31.25|         31.709999|111455800|         24.801661|
|    BP|2010-06-16|              29.9|              33.0|              29.58|             31.85|235447100|         24.911162|
|    BP|2010-06-15|             30.58|         32.130001|              29.85|              31.4|165557000|24.559198000000002|
|    BP|2010-06-14|32.389998999999996|         32.599998| 30.559998999999998|             30.67|112737300|23.988235999999997|
|    BP|2010-06-11|         34.049999|34.459998999999996|              33.25|         33.970001|132861900|26.569298999999997|
|    BP|2010-06-10|             32.16|33.040001000000004|          30.889999|         32.779999|222397900|         25.638551|
|    BP|2010-06-09|33.959998999999996|         34.450001|               29.0|         29.200001|240808500|         22.838491|
|    BP|2010-06-01|             37.34|         38.529999|          36.200001|             36.52|120257000|         28.563756|
|    BP|2010-05-03|49.360001000000004|51.290001000000004|          47.349998|         50.189999|156810500|         39.255609|
|    BP|1993-11-23|             60.75|              61.5|             60.625|              61.5|124378800|6.6373690000000005|
|    BP|1993-08-25|              56.5|            57.375|               56.5|            57.125|107000800|6.1185160000000005|
|   EMC|2015-10-12|             28.75|             28.77|              27.32|             28.35|117641900|             28.35|
|   EMC|2013-01-29|             23.65|         24.389999|          23.110001|             24.18|133486800|23.194285999999998|
+------+----------+------------------+------------------+-------------------+------------------+---------+------------------+
only showing top 20 rows


In [17]:

stocks_df.describe().show()

+-------+--------------------+--------------------+--------------------+--------------------+------------------+------------------+
|summary|                Open|                High|                 Low|               Close|            Volume|         Adj Close|
+-------+--------------------+--------------------+--------------------+--------------------+------------------+------------------+
|  count|             2684844|             2684844|             2684844|             2684844|           2684844|           2684844|
|   mean|  31.678027824375906|  32.037832410594426|  31.310445329660354|   31.68516317086478|1182455.6487453275|22.132030689904614|
| stddev|   52.08746979013818|   52.35698350845711|   51.82291643362695|   52.10560145592166| 6416516.289179395| 52.15609447109511|
|    min|0.015625999999999998|0.015625999999999998|0.015625999999999998|0.015625999999999998|                 0|          0.022976|
|    max|         8649.999733|         8649.999733|         8649.999733|         8649.999733|        1226791300|2335.1906129999998|
+-------+--------------------+--------------------+--------------------+--------------------+------------------+------------------+


Ok so I actually want the company name as the first column in the stocks df data set. Then I need to save those changes.
In [8]:

spark_df.registerTempTable('tickerListing')

In [9]:

joined = stocks_df.join(spark_df, stocks_df.Ticker==spark_df.Ticker ,'inner').select(spark_df.Name,stocks_df.Ticker,'Category Name', stocks_df.Date, stocks_df.Open,stocks_df.Close, stocks_df.High, stocks_df.Low, stocks_df.Volume, "Adj Close")

In [10]:

joined.show()

+------------------+------+-------------------+----------+------------------+------------------+------------------+------------------+-------+------------------+
|              Name|Ticker|      Category Name|      Date|              Open|             Close|              High|               Low| Volume|         Adj Close|
+------------------+------+-------------------+----------+------------------+------------------+------------------+------------------+-------+------------------+
|Agree Realty Corp.|   ADC|Property Management|2015-12-10|         33.060001|         32.779999|         33.279999|         32.720001| 109300|         32.779999|
|Agree Realty Corp.|   ADC|Property Management|2015-12-09|         32.549999|             32.93|33.110001000000004|             32.52| 177400|             32.93|
|Agree Realty Corp.|   ADC|Property Management|2015-12-08|32.040001000000004|32.639998999999996|             33.07|              32.0|1276400|32.639998999999996|
|Agree Realty Corp.|   ADC|Property Management|2015-12-07|         33.380001|             33.48|              33.5|             33.02|  80600|             33.48|
|Agree Realty Corp.|   ADC|Property Management|2015-12-04|             33.07|         33.369999|         33.599998|             33.07|  63000|         33.369999|
|Agree Realty Corp.|   ADC|Property Management|2015-12-03|              33.5|         32.970001|         33.529999|         32.919998|  90800|         32.970001|
|Agree Realty Corp.|   ADC|Property Management|2015-12-02|             33.68|33.459998999999996|         33.919998|         33.400002|  94000|33.459998999999996|
|Agree Realty Corp.|   ADC|Property Management|2015-12-01|33.610001000000004|         33.830002|         33.900002|         33.439999|  58900|         33.830002|
|Agree Realty Corp.|   ADC|Property Management|2015-11-30|         33.810001|33.540001000000004|             33.98|33.360001000000004|  71200|33.540001000000004|
|Agree Realty Corp.|   ADC|Property Management|2015-11-27|         33.400002|             33.68|         33.779999|         33.400002|  33300|             33.68|
|Agree Realty Corp.|   ADC|Property Management|2015-11-25|33.259997999999996|         33.349998|             33.43|33.009997999999996|  29600|         33.349998|
|Agree Realty Corp.|   ADC|Property Management|2015-11-24|         32.939999|33.290001000000004|             33.34|32.610001000000004|  66900|33.290001000000004|
|Agree Realty Corp.|   ADC|Property Management|2015-11-23|             33.18|33.110001000000004|             33.32|             32.93|  55100|33.110001000000004|
|Agree Realty Corp.|   ADC|Property Management|2015-11-20|             32.98|             33.16|33.290001000000004|         32.919998|  64800|             33.16|
|Agree Realty Corp.|   ADC|Property Management|2015-11-19|32.889998999999996|         32.830002|         33.049999|         32.549999|  51900|         32.830002|
|Agree Realty Corp.|   ADC|Property Management|2015-11-18|         32.830002|         32.810001|         32.900002|32.459998999999996|  81300|         32.810001|
|Agree Realty Corp.|   ADC|Property Management|2015-11-17|32.639998999999996|32.740002000000004|              33.0|32.509997999999996|  47300|32.740002000000004|
|Agree Realty Corp.|   ADC|Property Management|2015-11-16|         32.189999|32.740002000000004|             32.77|32.139998999999996|  61400|32.740002000000004|
|Agree Realty Corp.|   ADC|Property Management|2015-11-13|             32.23|         32.299999|             32.59|             32.16|  83900|         32.299999|
|Agree Realty Corp.|   ADC|Property Management|2015-11-12|         32.549999|             32.43|32.860001000000004|             32.27|  66900|             32.43|
+------------------+------+-------------------+----------+------------------+------------------+------------------+------------------+-------+------------------+
only showing top 20 rows


In [71]:

joined.columns

Out[71]:

['Name',
 'Ticker',
 'Category Name',
 'Date',
 'Open',
 'Close',
 'High',
 'Low',
 'Volume',
 'Adj Close']

In [11]:

joined2 = joined.withColumnRenamed('Category Name','Category_Names').withColumnRenamed('Adj Close','Adj_Close')

In [12]:

joined2.head()

Out[12]:

Row(Name=u'Agree Realty Corp.', Ticker=u'ADC', Category_Names=u'Property Management', Date=u'2015-12-10', Open=33.060001, Close=32.779999, High=33.279999, Low=32.720001, Volume=109300, Adj_Close=32.779999)

In [13]:

joined2.write.save('stocks_df')

In [15]:

df = sqlContext.read.load('/home/bluerubez/Downloads/spark-1.5.2/stocks_df')

In [16]:

df.show()

+--------------------+------+-----------------+----------+------------------+------------------+------------------+------------------+------+------------------+
|                Name|Ticker|   Category_Names|      Date|              Open|             Close|              High|               Low|Volume|         Adj_Close|
+--------------------+------+-----------------+----------+------------------+------------------+------------------+------------------+------+------------------+
|Carriage Services...|   CSV|Personal Services|2015-12-10|             24.42|24.280001000000002|24.629998999999998|             24.23| 53200|24.280001000000002|
|Carriage Services...|   CSV|Personal Services|2015-12-09|             24.75|         24.360001|              25.0|             24.35|122100|         24.360001|
|Carriage Services...|   CSV|Personal Services|2015-12-08|             24.98|             24.77|25.190001000000002|             24.73| 61400|             24.77|
|Carriage Services...|   CSV|Personal Services|2015-12-07|             25.59|             25.23|25.690001000000002|25.059998999999998| 82100|             25.23|
|Carriage Services...|   CSV|Personal Services|2015-12-04|             24.68|             25.59|25.780001000000002|             24.66|143400|             25.59|
|Carriage Services...|   CSV|Personal Services|2015-12-03|25.559998999999998|             24.67|         25.790001|24.629998999999998|164300|             24.67|
|Carriage Services...|   CSV|Personal Services|2015-12-02|             24.84|25.559998999999998|         25.959999|             24.84|201300|25.559998999999998|
|Carriage Services...|   CSV|Personal Services|2015-12-01|             24.93|             24.84|             24.93|             24.58|108100|             24.84|
|Carriage Services...|   CSV|Personal Services|2015-11-30|             24.93|24.690001000000002|             25.02|             24.65|143800|24.690001000000002|
|Carriage Services...|   CSV|Personal Services|2015-11-27|         24.959999|         24.790001|25.129998999999998|             24.73| 85100|         24.790001|
|Carriage Services...|   CSV|Personal Services|2015-11-25|             24.91|24.940001000000002|             25.17|         24.889999| 90700|24.940001000000002|
|Carriage Services...|   CSV|Personal Services|2015-11-24|24.379998999999998|24.870001000000002|             25.17|24.129998999999998|194900|24.870001000000002|
|Carriage Services...|   CSV|Personal Services|2015-11-23|23.969998999999998|              24.5|             24.75|23.879998999999998|279000|              24.5|
|Carriage Services...|   CSV|Personal Services|2015-11-20|             24.09|             24.09|24.370001000000002|             23.98|129200|             24.09|
|Carriage Services...|   CSV|Personal Services|2015-11-19|24.870001000000002|         23.950001|24.870001000000002|23.870001000000002|126800|         23.950001|
|Carriage Services...|   CSV|Personal Services|2015-11-18|             24.32|24.879998999999998|         24.959999|             24.27|201500|24.879998999999998|
|Carriage Services...|   CSV|Personal Services|2015-11-17|23.719998999999998|              24.4|             24.51|23.620001000000002|221100|              24.4|
|Carriage Services...|   CSV|Personal Services|2015-11-16|              23.5|23.719998999999998|23.780001000000002|             23.23| 82300|23.719998999999998|
|Carriage Services...|   CSV|Personal Services|2015-11-13|         23.040001|23.620001000000002|             23.82|22.879998999999998|115200|23.620001000000002|
|Carriage Services...|   CSV|Personal Services|2015-11-12|         23.610001|             23.15|23.629998999999998|         23.139999|159200|             23.15|
+--------------------+------+-----------------+----------+------------------+------------------+------------------+------------------+------+------------------+
only showing top 20 rows


There is much more to learn in Apache Spark but i have gotten through the basics. I must say it is a lot easier to just do it then it is to read about it, I learned a lot more this way. I will continue this project into next semester and will further enrich the dataset with calculated values. Also I will set up a "backtesting" platform so when i come up with a trading algo I will be able to test it to see how sucessful it is. Also would like to incorporate RSS feeds to see how the market reacts to current financial news. This Chipotle food poisoining scare would have been a perfect example. 
